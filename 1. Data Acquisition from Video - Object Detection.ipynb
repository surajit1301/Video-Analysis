{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Vehicles from Video data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook performs the task of **taking a video file as input**, and creating **cropped images of the objects detected in the video**.\n",
    "\n",
    "These cropped images are created as training data for a Deep Learning model. The images will be further labeled into 3 classes - \n",
    "1. 2 or 3 wheelers (motorbikes, rickshaws, etc.)\n",
    "2. 4 wheelers (cars)\n",
    "3. 4+ wheelers (buses, trucks, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall workflow of this demonstration is as follows:\n",
    "\n",
    "- We first define some **global variables** that will be used through the entire demo\n",
    "- Then, we **define functions** that will be used during execution.\n",
    "- As soon as you run the **while loop** in the main function, the **first frame of the video is initialised**.\n",
    "- Using your mouse, you will **draw a line across one side of the road**. This will appear as a yellow line.\n",
    "- Once you draw the line, the video will start running.\n",
    "- Then, the video will keep running, and **vehicle images will get cropped** and get saved on a specified path.\n",
    "- When the **vehicle count reaches a certain threshold**, the video will stop.\n",
    "- All the cropped images will get saved on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run these if OpenCV doesn't load\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/cv2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Particle filter, KALMAN filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining few global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is created so that we can print necessary values while debugging.\n",
    "# Creating this variable is good general practice.\n",
    "SHOW_DEBUG_STEPS = False\n",
    "\n",
    "# This is a boolean variable which decides if a line has been dragged across or not\n",
    "drag = 0\n",
    "\n",
    "# This decides if a point has been selected or not\n",
    "select_flag = 0\n",
    "\n",
    "# These two points will be the endpoints of the line that we draw\n",
    "x1=0\n",
    "y1=0\n",
    "x2=1\n",
    "y2=1\n",
    "point1 = [x1,y1]\n",
    "point2 = [x2,y2]\n",
    "\n",
    "# This is a matrix version of the same two points\n",
    "crossingLine = np.zeros((2,2),np.int)\n",
    "crossingLine[0] = point1\n",
    "crossingLine[1] = point2\n",
    "\n",
    "# This is a boolean variable which determines if the frame is the first frame or not\n",
    "blnfFrame = True\n",
    "\n",
    "# Frame count is initialised (This is not initialised to 1 - we throw away the first frame after we draw a line on it)\n",
    "frameCnt = 2\n",
    "\n",
    "# This is a variable that's evoked when we are drawing the line on the frame\n",
    "callback = False\n",
    "\n",
    "# The count of vehicles is initialised to 0\n",
    "vehicleCount = 0\n",
    "\n",
    "# This is the title of the window where the video will play\n",
    "src_window='Vehicle Counting - Blob Save'\n",
    "\n",
    "# This is a variable that keeps track of how many blobs have been cropped\n",
    "u=0\n",
    "\n",
    "# Here, we define some colours\n",
    "SCALAR_BLACK = (0.0,0.0,0.0)\n",
    "SCALAR_WHITE = (255.0,255.0,255.0)\n",
    "SCALAR_YELLOW = (0.0,255.0,255.0)\n",
    "SCALAR_GREEN = (0.0,255.0,0.0)\n",
    "SCALAR_RED = (0.0,0.0,255.0)\n",
    "SCALAR_CYAN = (255.0,255.0,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the blob class\n",
    "- Defining the class\n",
    "- Writing functions to define parameters of a blob like center, diagonal, etc\n",
    "- Predicting the next position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    currentContour = [[0,0]]\n",
    "    x=0\n",
    "    y=0\n",
    "    w=0\n",
    "    h=0\n",
    "    \n",
    "    currentBoundingRect = [x,y,w,h] # = cv2.boundingRect(_contour)\n",
    "    centerPositions = [0,0] ## Center size of the rectangle that hold the blob\n",
    "    \n",
    "    dblCurrentDiagonalSize = 0.0 ## Diagonal size of the rectangle that hold the blob\n",
    "    dblCurrentAspectRatio = 0.0\n",
    "    \n",
    "    blnCurrentMatchFoundOrNewBlob = False ## To Control the multiple count of same vehicle\n",
    "    \n",
    "    blnStillBeingTracked = False ## How many consequetive frames is a blob continuously counted\n",
    "    \n",
    "    intNumOfConsecutiveFramesWithoutAMatch = 0 ## If a blob from last frame coming into the same frame\n",
    "    \n",
    "    predictedNextPosition = np.zeros((1,2),np.int)\n",
    "    \n",
    "    # First, let's define the 'Blob' function, which creates a 'Blob', with all the necessary parameters\n",
    "    \n",
    "    def Blob(self,_contour):\n",
    "        self.currentContour = _contour\n",
    "        self.currentBoundingRect = cv2.boundingRect(_contour)\n",
    "        currentCenter = [0,0]\n",
    "        \n",
    "        # This variable defines the center of the blob\n",
    "        currentCenter[0] = self.currentBoundingRect[0] + self.currentBoundingRect[2] / 2\n",
    "        currentCenter[1] = self.currentBoundingRect[1] + self.currentBoundingRect[3] / 2\n",
    "        \n",
    "        # This is a list of all the center positions of one blob\n",
    "        self.centerPositions.append(currentCenter)\n",
    "        \n",
    "        ## Calculating the diagonal length and aspect to check if it is a valid blob in currentBlobs list\n",
    "        self.dblCurrentDiagonalSize = math.sqrt(math.pow(self.currentBoundingRect[2],2) + math.pow(self.currentBoundingRect[3],2))\n",
    "        self.dblCurrentAspectRatio = float(self.currentBoundingRect[2]) / float(self.currentBoundingRect[3]) \n",
    "        \n",
    "        ## Initializing few variables that will be used for tracking\n",
    "        self.blnStillBeingTracked = True  ## This will be changed if matching is not successful\n",
    "        self.blnCurrentMatchFoundOrNewBlob = True\n",
    "        self.intNumOfConsecutiveFramesWithoutAMatch = 0 ## To consider dropping blob from tracking\n",
    "        \n",
    "    \n",
    "    # Next, we define a function that predicts the next position of the blob\n",
    "    # This function takes the position of blobs detected in a frame\n",
    "    # Then it predicts the position of that same blob in subsequent\n",
    "        \n",
    "    def predictNextPosition(self):\n",
    "        numPositions = int(len(self.centerPositions))\n",
    "                    \n",
    "        ## Collection of center positions for a blob. It will have multiple center positions for different frames\n",
    "        if(numPositions==1): ## New center is same as previous center\n",
    "            self.predictedNextPosition[0][0]=self.centerPositions[-1][0] # or [-1:] ## X coordinate\n",
    "            self.predictedNextPosition[0][1]=self.centerPositions[-1][1] ## y coordinate\n",
    "            \n",
    "        elif(numPositions==2):\n",
    "            deltaX = self.centerPositions[1][0] - self.centerPositions[0][0]\n",
    "            deltaY = self.centerPositions[1][1] - self.centerPositions[0][1]\n",
    "            \n",
    "            self.predictedNextPosition[0][0] = self.centerPositions[-1][0] + deltaX # or [-1:]\n",
    "            self.predictedNextPosition[0][1] = self.centerPositions[-1][1] + deltaY\n",
    "        \n",
    "        elif(numPositions==3):\n",
    "            sumOfXChanges = ((self.centerPositions[2][0] - self.centerPositions[1][0])*2) + ((self.centerPositions[1][0] - self.centerPositions[0][0])*1)\n",
    "            deltaX = int(round(float(sumOfXChanges/3.0)+0.5))\n",
    "            \n",
    "            sumOfYChanges = ((self.centerPositions[2][1] - self.centerPositions[1][1]) * 2) + ((self.centerPositions[1][1] - self.centerPositions[0][1]) * 1)\n",
    "            deltaY = int(round(float(sumOfYChanges/3.0)+0.5))\n",
    "            \n",
    "            self.predictedNextPosition[0][0]=self.centerPositions[-1][0] + deltaX # or [-1:]\n",
    "            self.predictedNextPosition[0][1]=self.centerPositions[-1][1] + deltaY\n",
    "        \n",
    "        elif(numPositions==4):\n",
    "            sumOfXChanges = ((self.centerPositions[3][0] - self.centerPositions[2][0]) * 3) + ((self.centerPositions[2][0] - self.centerPositions[1][0]) * 2) + ((self.centerPositions[1][0] - self.centerPositions[0][0]) * 1)\n",
    "            deltaX = int(round(float(sumOfXChanges/6.0)+0.5))\n",
    "            \n",
    "            sumOfYChanges = ((self.centerPositions[3][1] - self.centerPositions[2][1]) * 3) + ((self.centerPositions[2][1] - self.centerPositions[1][1]) * 2) + ((self.centerPositions[1][1] - self.centerPositions[0][1]) * 1)\n",
    "            deltaY = int(round(float(sumOfYChanges/6.0)+0.5))\n",
    "            \n",
    "            self.predictedNextPosition[0][0]=self.centerPositions[-1][0] + deltaX # or [-1:]\n",
    "            self.predictedNextPosition[0][1]=self.centerPositions[-1][1] + deltaY\n",
    "        \n",
    "        elif(numPositions>=5):\n",
    "            sumOfXChanges = ((self.centerPositions[numPositions-1][0] - self.centerPositions[numPositions-2][0]) * 4) + ((self.centerPositions[numPositions-2][0] - self.centerPositions[numPositions-3][0]) * 3) + ((self.centerPositions[numPositions-3][0] - self.centerPositions[numPositions-4][0]) * 2) + ((self.centerPositions[numPositions-4][0] - self.centerPositions[numPositions-5][0]) * 1)\n",
    "            deltaX = int(round(float(sumOfXChanges/10.0)+0.5))\n",
    "            \n",
    "            sumOfYChanges = ((self.centerPositions[numPositions-1][1] - self.centerPositions[numPositions-2][1]) * 4) + ((self.centerPositions[numPositions-2][1] - self.centerPositions[numPositions-3][1]) * 3) + ((self.centerPositions[numPositions-3][1] - self.centerPositions[numPositions-4][1]) * 2) + ((self.centerPositions[numPositions-4][1] - self.centerPositions[numPositions-5][1]) * 1)\n",
    "            deltaY = int(round(float(sumOfYChanges/10.0)+0.5))\n",
    "            \n",
    "            self.predictedNextPosition[0][0]=self.centerPositions[-1][0] + deltaX # or [-1:]\n",
    "            self.predictedNextPosition[0][1]=self.centerPositions[-1][1] + deltaY\n",
    "        else:\n",
    "            print (\"Shouldn't come here\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting contours ------------------------------------------------------------------\n",
    "def drawAndShowContours(wd,ht,contours,strImgName):\n",
    "    global SCALAR_WHITE\n",
    "    global SHOW_DEBUG_STEPS\n",
    "    blank_image = np.zeros((ht,wd,3), np.uint8)\n",
    "    cv2.drawContours(blank_image,contours,-1,SCALAR_WHITE,-1)\n",
    "    \n",
    "    if(SHOW_DEBUG_STEPS):\n",
    "        cv2.imshow(strImgName,blank_image)\n",
    "    return blank_image\n",
    "\n",
    "## Plotting blobs ------------------------------------------------------------------\n",
    "def drawAndShowBlobs(wd,ht,blobs,strImgName):\n",
    "    global SCALAR_WHITE\n",
    "    global SHOW_DEBUG_STEPS\n",
    "    blank_image = np.zeros((ht,wd,3), np.uint8)\n",
    "    \n",
    "    contours=[]\n",
    "    for blob in blobs:\n",
    "        if blob.blnStillBeingTracked == True:\n",
    "            contours.append(blob.currentContour)\n",
    "            \n",
    "    cv2.drawContours(blank_image,contours,-1,SCALAR_WHITE,-1)\n",
    "    \n",
    "    if(SHOW_DEBUG_STEPS):\n",
    "        cv2.imshow(strImgName,blank_image)\n",
    "    return blank_image\n",
    "\n",
    "## Drawing the blob we spot, onto an image ---------------------------------     \n",
    "def drawBlobInfoOnImage(blobs, imgFrame2Copy, imgorg, frmCnt, fps):\n",
    "    ## imgorg is copy of imgFrame2Copy\n",
    "    \n",
    "    global SCALAR_RED ## Red rectangle around the object     \n",
    "    global u    ## u = 0 Initializing\n",
    "    im_name = \"_\"\n",
    "    path = \"/Users/jaideepkhare/Documents/neural-networks/vehicle-detection/saved_images/\"    \n",
    "    \n",
    "    im_names = str(u)+'.jpg\\n'\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if blob.blnStillBeingTracked == True:\n",
    "            x,y,w,h = blob.currentBoundingRect\n",
    "            cv2.rectangle(imgFrame2Copy, (x,y),(x+w,y+h), SCALAR_RED, 2) \n",
    "            condi = frmCnt%5 ## 5th frame count\n",
    "            if (condi == 0):\n",
    "                img_1boundingbox = imgorg[y:y+h,x:x+w]\n",
    "                im_name = path+str(u)+\".jpg\"\n",
    "                resizedRoI = cv2.resize(img_1boundingbox, (50,50)) ## Fixed size for all cropped images\n",
    "                cv2.imwrite(im_name,resizedRoI)\n",
    "                if not(u==0):\n",
    "                    im_names = im_names + str(u)+'.jpg\\n'\n",
    "                u = u+1                \n",
    "                \n",
    "#     for i in range (len(blobs)):\n",
    "#         if blobs[i].blnStillBeingTracked == True:\n",
    "#             x,y,w,h = blobs[i].currentBoundingRect\n",
    "#             cv2.rectangle(imgFrame2Copy, (x,y),(x+w,y+h), SCALAR_RED, 2)\n",
    "#             #if ((frmCnt-2)%(fps+1) == 0):\n",
    "#             img_1boundingbox = imgorg[y:y+h,x:x+w]\n",
    "#             im_name = path+str(u)+\".jpg\"\n",
    "#             img_1boundingbox = cv2.resize(img_1boundingbox, (50,50))\n",
    "#             cv2.imwrite(im_name,img_1boundingbox)\n",
    "#             if not(u==0):\n",
    "#                 im_names = im_names + str(u)+'.jpg\\n'\n",
    "#             u = u+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ DONE ###############################################        \n",
    "# This function is used to draw the yellow line on the first frame.\n",
    "# This yellow line acts as the boundary - if vehicles cross this line, they are counted as a blob\n",
    "## 8 : Neighbours to consider for Anti-aliasing\n",
    "## 2 : Thickness of the line\n",
    "## Tuples of two points incicating the start and fnish of the line\n",
    "    \n",
    "def drawMyLine(frame):\n",
    "    global point1\n",
    "    global point2\n",
    "    global SCALAR_YELLOW\n",
    "    cv2.line(frame,(point1[0],point1[1]),(point2[0],point2[1]),SCALAR_YELLOW,2,8) \n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "################################ DONE ###############################################\n",
    "# This mouse performs the actual drawing of the line\n",
    "def myMouseHandler(event,x,y,flags,param): # Click left button to start RoI selection\n",
    "    global point1\n",
    "    global point2\n",
    "    \n",
    "    ## Drag = 0\n",
    "    global drag\n",
    "    \n",
    "    ## Select Flag = 0\n",
    "    global select_flag\n",
    "    \n",
    "    global callback ## FALSE\n",
    "    \n",
    "    ## Initiating the event through mouse\n",
    "    if (event == cv2.EVENT_LBUTTONDOWN and not(drag) and not(select_flag)):\n",
    "        point1 = [x,y]\n",
    "        drag = 1\n",
    "    \n",
    "    if (event == cv2.EVENT_MOUSEMOVE and drag and not(select_flag)): # Drag mouse to select RoI\n",
    "        img1 = fFrame.copy()\n",
    "        print ('img1 height' + str(img1.shape[0]))\n",
    "        print ('img1 width' + str(img1.shape[1]))\n",
    "        point2 = [x,y]\n",
    "        \n",
    "        ## Drawing a line in the fFrame between point 1 and point 2\n",
    "        drawMyLine(fFrame)\n",
    "        \n",
    "        cv2.imshow(src_window,img1) # why img1?\n",
    "        \n",
    "    if(event == cv2.EVENT_LBUTTONUP and drag and not(select_flag)): # Complete selection\n",
    "        img2 = fFrame.copy()\n",
    "        print ('img2 height' + str(img2.shape[0]))\n",
    "        print ('img2 width' + str(img2.shape[1]))\n",
    "        point2 = [x,y] ## Coordinates where the mouse is released\n",
    "        \n",
    "        drag = 0\n",
    "        select_flag = 1\n",
    "        \n",
    "        cv2.imshow(src_window,img2) # why img2?\n",
    "        callback = 1\n",
    "##################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funtion checks if a vehicle is to the left of the line that we have drawn    \n",
    "def isLeftOfLineAB(a,b,c):\n",
    "    return ( (b[0]-a[0])*(c[1]-a[1]) - (b[1]-a[1])*(c[0]-a[0]) ) > 0\n",
    "       \n",
    "        \n",
    "## For the blobs in existing blobs that got matched with the current blob, update the properties of that blob\n",
    "## to match the current blob\n",
    "def addBlobToExistingBlobs(currentFrameBlob, existingBlobs, intIndex):\n",
    "    existingBlobs[intIndex].currentContour = currentFrameBlob.currentContour\n",
    "    existingBlobs[intIndex].currentBoundingRect = currentFrameBlob.currentBoundingRect\n",
    "\n",
    "    existingBlobs[intIndex].centerPositions.append(currentFrameBlob.centerPositions[-1])\n",
    "\n",
    "    existingBlobs[intIndex].dblCurrentDiagonalSize = currentFrameBlob.dblCurrentDiagonalSize\n",
    "    existingBlobs[intIndex].dblCurrentAspectRatio = currentFrameBlob.dblCurrentAspectRatio\n",
    "\n",
    "    existingBlobs[intIndex].blnStillBeingTracked = True ## Intention to keep track in future\n",
    "    existingBlobs[intIndex].blnCurrentMatchFoundOrNewBlob = True\n",
    "\n",
    "## Euclidean distance --------------------------    \n",
    "def distanceBetweenPoints(point1, point2):\n",
    "    if(SHOW_DEBUG_STEPS):\n",
    "        print ('point1: '+str(type(point1))+'point1: '+str(point1))\n",
    "        print ('point2: '+str(type(point2))+'point2: '+str(point2))\n",
    "    intX = abs(point1[0] - point2[0][0])\n",
    "    intY = abs(point1[1] - point2[0][1])\n",
    "\n",
    "    return (math.sqrt(math.pow(intX, 2) + math.pow(intY, 2)))\n",
    "\n",
    "\n",
    "########################### MATCHING FUNCTION ###############################################\n",
    "def matchCurrentFrameBlobsToExistingBlobs(existingBlobs, currentFrameBlobs):\n",
    "    \n",
    "    for i in range (len(existingBlobs)):\n",
    "        ## Initializing the tracking parameter to False, will be updated after matching\n",
    "        existingBlobs[i].blnCurrentMatchFoundOrNewBlob =  False\n",
    "        existingBlobs[i].predictNextPosition()    \n",
    "        \n",
    "    for currentFrameBlob in currentFrameBlobs:\n",
    "        intIndexOfLeastDistance = 0 ## Index of the blob that has the least distance with current blob\n",
    "        dblLeastDistance = 100000.0 ## What is that minimum distance\n",
    "        \n",
    "        ## Looping through existing blobs to find the blob with which it has the least distance\n",
    "        for i in range(len(existingBlobs)):\n",
    "            if existingBlobs[i].blnStillBeingTracked == True:\n",
    "                if(SHOW_DEBUG_STEPS):\n",
    "                    print (\"existingBlobs[i].predictedNextPosition: \" + str(type(existingBlobs[i].predictedNextPosition)))\n",
    "                    print (\"currentFrameBlob.centerPositions[-1]: \" + str(type(currentFrameBlob.centerPositions[-1])))\n",
    "                dblDistance = distanceBetweenPoints(currentFrameBlob.centerPositions[-1], existingBlobs[i].predictedNextPosition.tolist())\n",
    "                    \n",
    "                if dblDistance < dblLeastDistance:\n",
    "                    dblLeastDistance = dblDistance\n",
    "                    intIndexOfLeastDistance = i\n",
    "                \n",
    "        ## If condition satisfied, add the blob to existing blobs or create new blob            \n",
    "        ## Object is tracked if minimum distance is less than half the diagonal size\n",
    "        if dblLeastDistance < currentFrameBlob.dblCurrentDiagonalSize * 0.5: \n",
    "            addBlobToExistingBlobs(currentFrameBlob, existingBlobs, intIndexOfLeastDistance)\n",
    "        else:\n",
    "            addNewBlob(currentFrameBlob, existingBlobs) ## A new object has entered the frame\n",
    "    \n",
    "    ## Update the flags of that blob\n",
    "    for existingBlob in existingBlobs:\n",
    "        if existingBlob.blnCurrentMatchFoundOrNewBlob == False:\n",
    "            existingBlob.intNumOfConsecutiveFramesWithoutAMatch = existingBlob.intNumOfConsecutiveFramesWithoutAMatch + 1\n",
    "        if existingBlob.intNumOfConsecutiveFramesWithoutAMatch >= 5:\n",
    "            existingBlob.blnStillBeingTracked = False\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def addNewBlob(currentFrameBlob, existingBlobs):\n",
    "\n",
    "    currentFrameBlob.blnCurrentMatchFoundOrNewBlob = True ## This property will be copied to continue tracking this new \n",
    "    existingBlobs.append(currentFrameBlob)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def checkIfBlobsCrossedTheLine(blobs):\n",
    "    global point1\n",
    "    global point2\n",
    "    global vehicleCount\n",
    "    blnAtLeastOneBlobCrossedTheLine = False\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if (blob.blnStillBeingTracked == True and len(blob.centerPositions) >= 2):\n",
    "            prevFrmIdx = int(len(blob.centerPositions)) - 2\n",
    "            currFrmIdx = int(len(blob.centerPositions)) - 1\n",
    "            \n",
    "            if(SHOW_DEBUG_STEPS):\n",
    "                print ('prevFrmIdx: '+str(prevFrmIdx))\n",
    "                print ('currFrmIdx: '+str(currFrmIdx))\n",
    "                print ('blob.centerPositions: '+str(blob.centerPositions[prevFrmIdx][1]))\n",
    "            c=[1,2]\n",
    "            d=[1,2]\n",
    "            \n",
    "            d=blob.centerPositions[prevFrmIdx]\n",
    "            c=blob.centerPositions[currFrmIdx]\n",
    "            \n",
    "            ## Checking if point C & D which are center positions of blob lie left of line\n",
    "            ## They should lie on different sides of the line\n",
    "            if (isLeftOfLineAB(point1,point2,c) and not(isLeftOfLineAB(point1,point2,d))):\n",
    "                vehicleCount = vehicleCount + 1\n",
    "                blnAtLeastOneBlobCrossedTheLine = True\n",
    "    \n",
    "    return blnAtLeastOneBlobCrossedTheLine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to control the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blob = Blob()\n",
    "blobs = [blob]\n",
    "\n",
    "# This function closes all the windows that are open with either video or images in them\n",
    "def closeAll():\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "# This functions checks if the video is corrupted \n",
    "def retn(ret):\n",
    "    if not ret == True:\n",
    "        print(\"Error reading frame\")\n",
    "        closeAll()\n",
    "\n",
    "# This functions checks if the first frame is missing                \n",
    "def frm(frame):\n",
    "    if fFrame is None:\n",
    "        print(\"Error reading frame\")\n",
    "        closeAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################# PUTTING IT ALL TOGETHER  #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height720\n",
      "width1280\n"
     ]
    }
   ],
   "source": [
    "## Reading the Video\n",
    "cap = cv2.VideoCapture('/Users/jaideepkhare/Documents/neural-networks/vehicle-detection/AundhBridge.mp4')\n",
    "\n",
    "if not(cap.isOpened()):\n",
    "    print(\"Error reading file\")\n",
    "\n",
    "ret, fFrame  = cap.read()\n",
    "retn(ret)\n",
    "frm(fFrame)\n",
    "#fFrame = cv2.resize(fFrame, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "fGray = cv2.cvtColor(fFrame, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(src_window, fFrame)\n",
    "          \n",
    "ret, fFrame1 = cap.read()\n",
    "ret, fFrame2 = cap.read()\n",
    "print ('height' + str(fFrame1.shape[0]))\n",
    "print ('width' + str(fFrame1.shape[1]))\n",
    "\n",
    "#fFrame1 = cv2.resize(fFrame1, (0,0), fx=0.5, fy=0.5)\n",
    "#fFrame2 = cv2.resize(fFrame2, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "cv2.setMouseCallback(src_window,myMouseHandler)\n",
    "chChkEscKey = 0  ## Stoppping the processing of the video\n",
    "k = 27     ## ASCII character for Escape key\n",
    "blnFirstFrame = True ## For the first frame we display it, draw the line, Only from the second frame we start processing\n",
    "frameCnt = 2 ## 3 frames are taken uptill now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the while loop. It is kept open as long as the OpenCV video object is kept open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img1 height720\n",
      "img1 width1280\n",
      "img2 height720\n",
      "img2 width1280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while(cap.isOpened()):\n",
    "    if(callback): ## Denotes only after drawing of the line in the first frame we do further processing\n",
    "        currentFrameBlob = Blob()\n",
    "        currentFrameBlobs = [currentFrameBlob]\n",
    "        img1 = fFrame1.copy()\n",
    "        img2 = fFrame2.copy()\n",
    "        if(SHOW_DEBUG_STEPS):\n",
    "            print ('img1 height' + str(img1.shape[0]))\n",
    "            print ('img1 width' + str(img1.shape[1]))\n",
    "            print ('img2 height' + str(img2.shape[0]))\n",
    "            print ('img2 width' + str(img2.shape[1]))\n",
    "            \n",
    "            \n",
    "        ################# Starting pre-processing of the frames #################    \n",
    "        \n",
    "        # Convert the images to colour in order to enable fast processing\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Add some Gaussian Blur\n",
    "        img1 = cv2.GaussianBlur(img1,(5,5),0)\n",
    "        img2 = cv2.GaussianBlur(img2,(5,5),0)\n",
    "        \n",
    "        \n",
    "        # This imgDiff variable is the difference between consecutive frames, which is equivalent to detecting movement\n",
    "        imgDiff = cv2.absdiff(img1, img2) \n",
    "        \n",
    "        ret,imgThresh = cv2.threshold(imgDiff,30.0,255.0,cv2.THRESH_BINARY)\n",
    "        \n",
    "        ## Getting height and width of the differenced frames\n",
    "        ht = np.size(imgThresh,0)\n",
    "        wd = np.size(imgThresh,1)\n",
    "        \n",
    "        if(SHOW_DEBUG_STEPS):\n",
    "            cv2.imshow('imgThresh', imgThresh)\n",
    "        \n",
    "        ## Now, we define structuring elements for dilation and erosion\n",
    "        ## MORPH_LINE\n",
    "        ## MORPH_ELLIPSE\n",
    "        strucEle3x3 = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "        strucEle5x5 = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))\n",
    "        strucEle7x7 = cv2.getStructuringElement(cv2.MORPH_RECT,(7,7))        \n",
    "        strucEle15x15 = cv2.getStructuringElement(cv2.MORPH_RECT,(15,15))\n",
    "        \n",
    "        # Next, we perform dilation and erosion twice on this difference image\n",
    "        ## Good practice to keep dilation more than erosion\n",
    "        for i in range(2):\n",
    "            imgThresh = cv2.dilate(imgThresh,strucEle5x5,iterations = 2)\n",
    "            imgThresh = cv2.erode(imgThresh,strucEle5x5,iterations = 1)\n",
    "        \n",
    "        imgThreshCopy = imgThresh.copy()\n",
    "        \n",
    "        if(SHOW_DEBUG_STEPS):        \n",
    "            print ('imgThreshCopy height' + str(imgThreshCopy.shape[0]))\n",
    "            print ('imgThreshCopy width' + str(imgThreshCopy.shape[1]))\n",
    "                \n",
    "        # Creating contours\n",
    "        im, contours, hierarchy = cv2.findContours(imgThreshCopy,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        im2 = drawAndShowContours(wd,ht,contours,'imgContours')\n",
    "            \n",
    "        if(SHOW_DEBUG_STEPS):\n",
    "            print ('contours.shape: ' + str(len(contours)))\n",
    "               \n",
    "        # Next, we define hulls\n",
    "        for i in range(len(contours)):\n",
    "            hulls[i] = cv2.convexHull(contours[i])\n",
    "            \n",
    "        # Then we draw the contours\n",
    "        im3 = drawAndShowContours(wd,ht,hulls,'imgConvexHulls')\n",
    "        \n",
    "        # Next, we move to blobs\n",
    "        # First, each hull is passed through the Blob function            \n",
    "        for hull in hulls:\n",
    "            # This is an instance of the class Blob()\n",
    "            possiBlob = Blob()\n",
    "            # This is the Blob function inside the class Blob()\n",
    "            possiBlob.Blob(hull) # does it work? yes\n",
    "            currentBoundingRectArea = possiBlob.currentBoundingRect[2] * possiBlob.currentBoundingRect[3] #(Height * Width)\n",
    "            contourArea = cv2.contourArea(hull)\n",
    "            \n",
    "            # Determine if the hull is a valid blob\n",
    "            if(currentBoundingRectArea > 400 and possiBlob.dblCurrentAspectRatio > 0.2 and possiBlob.dblCurrentAspectRatio < 4.0 and possiBlob.currentBoundingRect[2] > 30 and possiBlob.currentBoundingRect[3] > 30 and possiBlob.dblCurrentDiagonalSize > 60.0 and (contourArea/int(currentBoundingRectArea) > 0.5)):\n",
    "                currentFrameBlobs.append(possiBlob)\n",
    "        \n",
    "        # Drawing the blob objects\n",
    "        im4 = drawAndShowBlobs(wd,ht,currentFrameBlobs,'imgCurrentFrameBlobs')\n",
    "        \n",
    "        ## If it is a first frame append it to the current list or we match it with existing blobs    \n",
    "        if blnFirstFrame ==  True:\n",
    "            for currFrameBlob in currentFrameBlobs:\n",
    "                blobs.append(currFrameBlob) ## We keep appending the blobs to our blobs list, it always grows in size\n",
    "        else:\n",
    "            matchCurrentFrameBlobsToExistingBlobs(blobs, currentFrameBlobs)\n",
    "        \n",
    "        im5 = drawAndShowBlobs(wd,ht, blobs, \"imgBlobs\")\n",
    "        \n",
    "        img2 = fFrame2.copy()\n",
    "        if(SHOW_DEBUG_STEPS):\n",
    "            print ('img2 height' + str(img2.shape[0]))\n",
    "            print ('img2 width' + str(img2.shape[1]))\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        drawBlobInfoOnImage(blobs, img2, fFrame2, frameCnt, fps)\n",
    "        \n",
    "        # Now, we assign the Boolean variable to whether the Blob has crossed the line we drew or not\n",
    "        blnAtLeastOneBlobCrossedTheLine = checkIfBlobsCrossedTheLine(blobs)\n",
    "        \n",
    "        ## PLotting the line on the second frame\n",
    "        if(SHOW_DEBUG_STEPS):\n",
    "            print ('type: ')\n",
    "            print (type(crossingLine[0]))\n",
    "        if blnAtLeastOneBlobCrossedTheLine == True:\n",
    "            cv2.line(img2, (point1[0],point1[1]),(point2[0],point2[1]),SCALAR_GREEN,2)\n",
    "        else:\n",
    "            cv2.line(img2, (point1[0],point1[1]),(point2[0],point2[1]),SCALAR_RED,2)\n",
    "            \n",
    "#         drawvehicleCountOnImage(img2)\n",
    "#         drawMyLine(img2) # is it necessary?\n",
    "        \n",
    "        cv2.imshow(src_window,img2) ## Showing the image with the line color\n",
    "        \n",
    "        ## clear list\n",
    "        currentFrameBlobs[:]=[] \n",
    "        \n",
    "        ## Moving ahead with the frames\n",
    "        fFrame1 = fFrame2.copy()\n",
    "        ret, fFrame2 = cap.read()\n",
    "        retn(ret)\n",
    "        frm(fFrame2)\n",
    "        \n",
    "        if(SHOW_DEBUG_STEPS):\n",
    "            print ('img1 height' + str(fFrame1.shape[0]))\n",
    "            print ('img1 width' + str(fFrame1.shape[1]))\n",
    "            print ('img2 height' + str(fFrame2.shape[0]))\n",
    "            print ('img2 width' + str(fFrame2.shape[1]))        \n",
    "        \n",
    "        blnFirstFrame = False\n",
    "        frameCnt = frameCnt + 1 ## Frame Count if required because we save blobs after every 5th frame\n",
    "    \n",
    "    k = cv2.waitKey(1) ## It will wait for a keyboard interrupt (Q or Escape key) \n",
    "    if k == 27 or k == ord('q') or vehicleCount > 1000:\n",
    "        closeAll()\n",
    "        break\n",
    "        \n",
    "closeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
